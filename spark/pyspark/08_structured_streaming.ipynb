{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Streaming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Philosophy of Structured Streaming:  \n",
    "* Unified programming model for batch and streaming\n",
    "* Broader definition of stream processing - from few hours to continuous processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming Model of Structured Streaming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Streams as an unbounded, continuously appended table\n",
    "\n",
    "![Data Stream as an unbounded input table](/home/thulasiram/personal/data_engineering/images/stream_tables.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Developers define query on the conceptual input table\n",
    "* Structured streaming will automatically convert this batch-like query to a streaming execution plan. This is called `incrementalization`\n",
    "* Developers specify triggers. Structured streaming checks for new data and incrementally updates the results\n",
    "  \n",
    "![Incrementalization](/home/thulasiram/personal/data_engineering/images/incremental.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five steps to define a streaming query:\n",
    "  * Define Input sources\n",
    "  * Transform data\n",
    "  * Define output sink and output mode\n",
    "  * Specify processing details\n",
    "  * Start the query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Define Input sources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use `spark.readStream` to create DataStreamReader\n",
    "* An example of creating a DF from a text data stream to be received from a socket connection\n",
    "* Spark natively supports reading data streams from Apache Kafka and all the various file-based formats  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 14:10:33 WARN Utils: Your hostname, thulasiram resolves to a loopback address: 127.0.1.1; using 192.168.0.105 instead (on interface wlp0s20f3)\n",
      "23/05/07 14:10:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 14:10:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 14:13:00 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "lines = (spark\n",
    "         .readStream.format(\"socket\")\n",
    "         .option(\"host\", \"localhost\")\n",
    "         .option(\"port\",9999)\n",
    "         .load()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Transform data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we can perform the usual DF operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the lines into individual words and count\n",
    "from pyspark.sql.functions import *\n",
    "words = lines.select(explode(split(col(\"value\"),\"\\\\s\")).alias(\"word\"))\n",
    "counts = words.groupBy(\"word\").count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `counts` is a streaming dataframe\n",
    "* word counts that will be computed once the streaming query is started\n",
    "* Two broad classes of data transformations:  \n",
    "  * `stateless transformations` - Does not require historical rows. Operations like `select()`, `filter()`, `map()`\n",
    "  * `stateful transformations` - require historical information. Operations like `grouping`, `joining` and `aggregations`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define output sink and output mode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* write the processed data with `DataFrame.writeStream`\n",
    "* Three output modes supported:  \n",
    "  * `append` - New rows appended since the last trigger will be written (Default mode). This is suitable for cases where output is never going to be changed or updated by the query in the future. This is only supported by the stateless queries\n",
    "  * `complete` - Completely overwrite an existing output stream. Suitable where results is much smaller than the input data and can be retained in memory\n",
    "  * `update` - Rows updated since the last trigger will be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the output stream to the console\n",
    "writer = counts.writeStream.format(\"console\").outputMode(\"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Specify Processing details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointDir = \".....\"\n",
    "writer2 = (\n",
    "    writer\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\",checkpointDir)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triggering Options:  \n",
    "* `Default` - Next micro-batch is triggered as soon as the prevous micro-batch has completed\n",
    "* `Processing time with trigger interval` - query will trigger micro-batches at specified fixed interval\n",
    "* `Once` - Execute exactly one micro-batch. Useful when we want to control the triggering and processing from an external scheduler\n",
    "* `continuous` - Process data continuously. Low latency than the micro-batch trigger modes\n",
    "  \n",
    "  \n",
    "Checkpoint Location - Location where the streaming query saves the progress information. Upon failure, this metadata is used to start from where it left off."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Start the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery = writer2.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "lines = (spark\n",
    "         .readStream.format(\"socket\")\n",
    "         .option(\"host\", \"localhost\")\n",
    "         .option(\"port\",9999)\n",
    "         .load()\n",
    "    )\n",
    "words = lines.select(explode(split(col(\"value\"), \"\\\\s\")).alias(\"word\"))\n",
    "counts = words.groupBy(\"word\").count()\n",
    "checkpointDir = `....`\n",
    "streamingQuery = (\n",
    "    counts\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\",checkpointDir)\n",
    "    .start()\n",
    ")\n",
    "streamingQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working of an Active Streaming Query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Working of a stream query](/home/thulasiram/personal/data_engineering/images/stream_query_working.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring the query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `StreamingQuery.lastProgress()` returns information on the last completed mirco-batch\n",
    "* `StreamingQuery.status()` returns what is happening at the moment\n",
    "* Spark supports [Dropwizard Metrics](https://metrics.dropwizard.io/4.2.0/). This library allows metrics to be published to many popular monitoring frameworks. If required they need to be configured by enabling `spark.sql.streaming.metricsEnabled` to `true` before starting your query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "inputDirectoryOfCSVFiles = ....\n",
    "\n",
    "fileSchema = (StructType()\n",
    ".add(StructField(\"key\",IntegerType()))\n",
    ".add(StructField(\"value\",IntegerType()))\n",
    ")\n",
    "\n",
    "inputDF = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"csv\")\n",
    "    .schema(fileSchema)\n",
    "    .load(inputDirectoryOfCSVFiles)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It only supports append mode because modifying existing data files is hard\n",
    "* It supports partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDir = .......\n",
    "checkpointDir = .......\n",
    "resultDF = .......\n",
    "\n",
    "streamingQuery = (\n",
    "    resultDF.writeStream\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\",outputDir)\n",
    "    .option(\"checkpointLocation\",checkpointDir)\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
